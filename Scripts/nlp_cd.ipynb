{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe135208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install googledrivedownloader\n",
    "#!pip install PyMuPDF\n",
    "#!pip install google-auth google-auth-oauthlib google-auth-httplib2\n",
    "#!pip install google-api-python-client\n",
    "#!pip install biopython\n",
    "#!pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb72275-05c5-43fc-9301-1d22a651741b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import os\n",
    "import PyPDF2\n",
    "import fitz\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "import io\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from Bio import Entrez,  Medline\n",
    "from docx import Document\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769dce0-2de9-4801-91ee-41f82955536a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e79f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory as \"final_project\"\n",
    "\n",
    "wd = '/Users/hammedadedamolaakande/Library/CloudStorage/OneDrive-ConcordiaUniversity-Canada/Data_Science_projects/Brave_Career/final_project'\n",
    "\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987361e",
   "metadata": {},
   "source": [
    "#### Download ALL the PDFs from the drive to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c523f-f0c2-483e-8228-186a5e48a7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This part is to download ALL the PDFs from the drive to local machine\n",
    "#Nothing to modify here except the folder ID (if its different from our current project, other remains the same)\n",
    "#Also, token should be deleted especially if you modify the SCOPES\n",
    "\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.json.\n",
    "#SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "def create_service():\n",
    "    creds = None\n",
    "    # token.json stores my access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, I'm gonna log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service\n",
    "\n",
    "# save service instance to interact with Google Drive\n",
    "service = create_service()\n",
    "\n",
    "\n",
    "\n",
    "def download_all_pdfs(service, folder_id):\n",
    "    page_token = None\n",
    "    while True:\n",
    "        # Query to list the PDF files in our Google Drive folder\n",
    "        results = service.files().list(\n",
    "            q=f\"'{folder_id}' in parents and mimeType='application/pdf'\",\n",
    "            pageSize=10,  # Keep as 10 (for fast download)\n",
    "            fields=\"nextPageToken, files(id, name)\",\n",
    "            pageToken=page_token  # Use the nextPageToken from the previous request\n",
    "        ).execute()\n",
    "\n",
    "        items = results.get('files', [])\n",
    "        if not items:\n",
    "            print('No files found.')\n",
    "            break  # Exit if no files are found\n",
    "\n",
    "        for item in items:\n",
    "            file_name = item['name']\n",
    "            # Check if the file already exists in the current directory\n",
    "            if not os.path.exists(file_name):\n",
    "                print(f\"Downloading {file_name}...\")\n",
    "                request = service.files().get_media(fileId=item['id'])\n",
    "                fh = io.BytesIO()\n",
    "                downloader = MediaIoBaseDownload(fh, request)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    status, done = downloader.next_chunk()\n",
    "                    print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "\n",
    "                # Save the file to disk only if it doesn't already exist\n",
    "                \n",
    "                with open(f'PDF_docs/{file_name}', 'wb') as f:\n",
    "                    fh.seek(0)\n",
    "                    f.write(fh.read())\n",
    "            else:\n",
    "                print(f\"{file_name} already exists. Skipping download.\")\n",
    "\n",
    "        page_token = results.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break \n",
    "\n",
    "            \n",
    "download_all_pdfs(service, '1n_9nLO15KnypYNEHbj9Z8USrBoQZVliQ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f603fc8-cada-4505-b186-a66b766a3dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea33d708",
   "metadata": {},
   "source": [
    "#### Convert the PDFs to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07864673-29b9-4de4-9cd5-ff35ee71b375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract data from the current wd- convert the PDFs to text\n",
    "\n",
    "\n",
    "def is_pdf_file(filename):\n",
    "    return filename.lower().endswith('.pdf')\n",
    "\n",
    "def convert_pdf_to_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def process_pdfs_in_directory(directory_path):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if is_pdf_file(filename):\n",
    "            full_path = os.path.join(directory_path, filename)\n",
    "            pdf_text = convert_pdf_to_text(full_path)\n",
    "            \n",
    "            # Define a new file name for the output with .txt extension\n",
    "            output_filename = filename.rsplit('.', 1)[0] + '.txt'\n",
    "            output_path = os.path.join(directory_path, output_filename)\n",
    "            \n",
    "            # Write the extracted text to a new text file\n",
    "            with open(output_path, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(pdf_text)\n",
    "            \n",
    "            print(f\"Processed and saved: {filename} as {output_filename}\")\n",
    "            \n",
    "# Set directory_path to the folder containing the PDFs\n",
    "directory_path = os.path.join(os.getcwd(), 'PDF_docs')\n",
    "process_pdfs_in_directory(directory_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8d4f6",
   "metadata": {},
   "source": [
    "#### Upload ALL the .txt files from the local machine to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47c033-eeef-47fb-a04a-4c22a6b6c588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This part is to upload ALL the .txt files from the local machine to our drive\n",
    "#Back transfer the text files to the Google Drive folder\n",
    "#Grant the drive API edit acess and not read only\n",
    "#Here, I advise you trash the token.json since we're using different SCOPES\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "#SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "def create_service():\n",
    "    creds = None\n",
    "    \n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, I'm gonna log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service\n",
    "\n",
    "# save service instance\n",
    "service_upload = create_service()\n",
    "\n",
    "\n",
    "\n",
    "def upload_all_txt_files(service, folder_id):\n",
    "    # Get the list of all files in the current working directory\n",
    "    cwd = os.getcwd()\n",
    "    files = [f for f in os.listdir(cwd) if os.path.isfile(os.path.join(cwd, f)) and f.endswith('.txt')]\n",
    "\n",
    "    # List all files in the Google Drive folder to check for duplicates\n",
    "    existing_files = service.files().list(q=f\"'{folder_id}' in parents\",\n",
    "                                          spaces='drive',\n",
    "                                          fields='nextPageToken, files(id, name)').execute()\n",
    "    existing_file_names = [file['name'] for file in existing_files.get('files', [])]\n",
    "\n",
    "    for file_name in files:\n",
    "        if file_name not in existing_file_names:\n",
    "            file_path = os.path.join(cwd, file_name)\n",
    "            file_metadata = {'name': file_name, 'parents': [folder_id]}\n",
    "            media = MediaFileUpload(file_path, mimetype='text/plain')\n",
    "            file = service.files().create(body=file_metadata,\n",
    "                                          media_body=media,\n",
    "                                          fields='id').execute()\n",
    "            print(f\"{file_name} uploaded successfully with File ID: {file.get('id')}\")\n",
    "        else:\n",
    "            print(f\"{file_name} already exists in the folder. Skipping upload.\")\n",
    "\n",
    "\n",
    "upload_all_txt_files(service_upload, '1n_9nLO15KnypYNEHbj9Z8USrBoQZVliQ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc33a4-6eb9-41ad-827f-956900163839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108612ef",
   "metadata": {},
   "source": [
    "#### Download ALL the .txt/docx files (not PDFs) from the drive to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is to download ALL the .txt/docx files (not PDFs) from the drive to local machine\n",
    "#Again,the folder ID should be the same (our present wd)\n",
    "\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "\n",
    "def create_service():\n",
    "    creds = None\n",
    "    # Recall that token.json stores our access and refresh tokens, and is created automatically when the authorization flow \n",
    "    # completes for the first time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, we can log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service\n",
    "\n",
    "\n",
    "# let's save service instance to interact with Google Drive\n",
    "service = create_service()\n",
    "\n",
    "\n",
    "def download_all_text_and_docx_files(service, folder_id):\n",
    "    page_token = None\n",
    "    while True:\n",
    "        # Query to list the text and docx files in the specified folder\n",
    "        results = service.files().list(\n",
    "            q=f\"'{folder_id}' in parents and (mimeType='text/plain' or mimeType='application/vnd.openxmlformats-officedocument.wordprocessingml.document' or mimeType='application/vnd.google-apps.document')\",\n",
    "            pageSize=10,  # Keep as 10 (for fast download)\n",
    "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
    "            pageToken=page_token  # Use the nextPageToken from the previous request\n",
    "        ).execute()\n",
    "\n",
    "        items = results.get('files', [])\n",
    "        if not items:\n",
    "            print('No files found.')\n",
    "            break  # Exit if no files are found\n",
    "\n",
    "        for item in items:\n",
    "            file_name = item['name']\n",
    "            file_mime_type = item['mimeType']\n",
    "            \n",
    "            # Ensure the file extension is correct for Google Docs files\n",
    "            if file_mime_type == 'application/vnd.google-apps.document':\n",
    "                file_name += '.docx'\n",
    "                request = service.files().export_media(fileId=item['id'], mimeType='application/vnd.openxmlformats-officedocument.wordprocessingml.document')\n",
    "            else:\n",
    "                request = service.files().get_media(fileId=item['id'])\n",
    "\n",
    "            # Check if the file already exists in the current directory\n",
    "            if not os.path.exists(file_name):\n",
    "                print(f\"Downloading {file_name}...\")\n",
    "                fh = io.BytesIO()\n",
    "                downloader = MediaIoBaseDownload(fh, request)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    status, done = downloader.next_chunk()\n",
    "                    print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "\n",
    "                # Let's save the file to disk only if it doesn't already exist\n",
    "                with open(f'txt_docs/{file_name}', 'wb') as f:\n",
    "                    fh.seek(0)\n",
    "                    f.write(fh.read())\n",
    "            else:\n",
    "                print(f\"{file_name} already exists. Skipping download.\")\n",
    "\n",
    "        page_token = results.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break  \n",
    "\n",
    "\n",
    "# Apply the function\n",
    "download_all_text_and_docx_files(service, '1n_9nLO15KnypYNEHbj9Z8USrBoQZVliQ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c45df4",
   "metadata": {},
   "source": [
    "### Convert docx to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba34f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docx_to_text(docx_file_path):\n",
    "    doc = Document(docx_file_path)\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def convert_all_docx_to_txt(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.docx'):\n",
    "            docx_file_path = os.path.join(folder_path, file_name)\n",
    "            text_file_name = file_name.replace('.docx', '.txt')\n",
    "            text_file_path = os.path.join(folder_path, text_file_name)\n",
    "\n",
    "            # Read the content from the .docx file\n",
    "            text_content = docx_to_text(docx_file_path)\n",
    "\n",
    "            # Write the content to a new .txt file\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(text_content)\n",
    "\n",
    "            print(f\"Converted {file_name} to {text_file_name}\")\n",
    "\n",
    "# Convert all .docx files in the 'txt_docs' folder to .txt files\n",
    "convert_all_docx_to_txt('txt_docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70474750-e0cf-4207-b686-33cce202dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can start working on the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834aa0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59efa876",
   "metadata": {},
   "source": [
    "### Upload Data to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70feec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data uploader\n",
    "\n",
    "import boto3\n",
    "\n",
    "def upload_to_s3(file_name, bucket, object_name=None, aws_access_key_id=None, aws_secret_access_key=None, region_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket using put_object\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified, file_name is used\n",
    "    :param aws_access_key_id: AWS access key ID\n",
    "    :param aws_secret_access_key: AWS secret access key\n",
    "    :param region_name: AWS region name\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "    \n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # Upload the file\n",
    "    with open(file_name, 'rb') as file_data:\n",
    "        s3_client.put_object(Bucket=bucket, Key=object_name, Body=file_data)\n",
    "    print(f\"File {file_name} uploaded to {bucket}/{object_name}\")\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5ba2275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./txt_docs/A1C Diabetes Test.txt uploaded to chatbot-pro/raw-data/A1C Diabetes Test.txt\n",
      "File ./txt_docs/Diabetes_symptoms_and_causes.txt uploaded to chatbot-pro/raw-data/Diabetes_symptoms_and_causes.txt\n",
      "File ./txt_docs/A1C_test_variants_and_ethnicity.txt uploaded to chatbot-pro/raw-data/A1C_test_variants_and_ethnicity.txt\n",
      "File ./txt_docs/Diabetes_brochure 5 15 2020.txt uploaded to chatbot-pro/raw-data/Diabetes_brochure 5 15 2020.txt\n",
      "File ./txt_docs/Managing_Diabetes.txt uploaded to chatbot-pro/raw-data/Managing_Diabetes.txt\n",
      "File ./txt_docs/Type_1_Diabetes.txt uploaded to chatbot-pro/raw-data/Type_1_Diabetes.txt\n",
      "File ./txt_docs/Diabetes_and_Digestion.txt uploaded to chatbot-pro/raw-data/Diabetes_and_Digestion.txt\n",
      "File ./txt_docs/Diabetes_Healthy_Living.txt uploaded to chatbot-pro/raw-data/Diabetes_Healthy_Living.txt\n",
      "File ./txt_docs/Diabetes_Management_Travelling.txt uploaded to chatbot-pro/raw-data/Diabetes_Management_Travelling.txt\n",
      "File ./txt_docs/Medicines.txt uploaded to chatbot-pro/raw-data/Medicines.txt\n",
      "File ./txt_docs/Diabetic_Diet.txt uploaded to chatbot-pro/raw-data/Diabetic_Diet.txt\n",
      "File ./txt_docs/Glycemic_Index.txt uploaded to chatbot-pro/raw-data/Glycemic_Index.txt\n",
      "File ./txt_docs/Changing_Diabetes_Habits.txt uploaded to chatbot-pro/raw-data/Changing_Diabetes_Habits.txt\n",
      "File ./txt_docs/Diabetes.txt uploaded to chatbot-pro/raw-data/Diabetes.txt\n",
      "File ./txt_docs/Prediabetes.txt uploaded to chatbot-pro/raw-data/Prediabetes.txt\n",
      "File ./txt_docs/A1C_Test.txt uploaded to chatbot-pro/raw-data/A1C_Test.txt\n",
      "File ./txt_docs/Diabetes_Nutrition.txt uploaded to chatbot-pro/raw-data/Diabetes_Nutrition.txt\n",
      "File ./txt_docs/Tests_And_Diagnoses.txt uploaded to chatbot-pro/raw-data/Tests_And_Diagnoses.txt\n",
      "File ./txt_docs/High_Sugar_Self_Care.txt uploaded to chatbot-pro/raw-data/High_Sugar_Self_Care.txt\n",
      "File ./txt_docs/Diabetes_Risk_Factors.txt uploaded to chatbot-pro/raw-data/Diabetes_Risk_Factors.txt\n",
      "File ./txt_docs/Diabetes_Type2_Meal_Planning.txt uploaded to chatbot-pro/raw-data/Diabetes_Type2_Meal_Planning.txt\n",
      "File ./txt_docs/Diabetes_Nutrition_Plate_Method.txt uploaded to chatbot-pro/raw-data/Diabetes_Nutrition_Plate_Method.txt\n",
      "File ./txt_docs/Diabetes_Recipes.txt uploaded to chatbot-pro/raw-data/Diabetes_Recipes.txt\n",
      "File ./txt_docs/familydoctor.org-Diabetes.txt uploaded to chatbot-pro/raw-data/familydoctor.org-Diabetes.txt\n",
      "File ./txt_docs/Diabetes_Nutrition_MealPlanning.txt uploaded to chatbot-pro/raw-data/Diabetes_Nutrition_MealPlanning.txt\n",
      "File ./txt_docs/Prediabetes_and_Insulin_Resistance.txt uploaded to chatbot-pro/raw-data/Prediabetes_and_Insulin_Resistance.txt\n",
      "File ./txt_docs/Diabetes_Nutrition_Carb_Counting.txt uploaded to chatbot-pro/raw-data/Diabetes_Nutrition_Carb_Counting.txt\n",
      "File ./txt_docs/Diabetes_RDN.txt uploaded to chatbot-pro/raw-data/Diabetes_RDN.txt\n",
      "File ./txt_docs/Diabetes_In_The_Elderly.txt uploaded to chatbot-pro/raw-data/Diabetes_In_The_Elderly.txt\n",
      "File ./txt_docs/Diabetes_Symptoms,Causes,Treatment.txt uploaded to chatbot-pro/raw-data/Diabetes_Symptoms,Causes,Treatment.txt\n",
      "File ./txt_docs/Diabetes_Nutrition_Snacking.txt uploaded to chatbot-pro/raw-data/Diabetes_Nutrition_Snacking.txt\n",
      "File ./txt_docs/Diabetes_and_Mental_Health.txt uploaded to chatbot-pro/raw-data/Diabetes_and_Mental_Health.txt\n",
      "File ./txt_docs/Blood Glucose Tests.txt uploaded to chatbot-pro/raw-data/Blood Glucose Tests.txt\n",
      "File ./txt_docs/What_Is_Diabetes.txt uploaded to chatbot-pro/raw-data/What_Is_Diabetes.txt\n",
      "File ./txt_docs/Diabetes_and_Heart_Disease.txt uploaded to chatbot-pro/raw-data/Diabetes_and_Heart_Disease.txt\n",
      "File ./txt_docs/Diabetes_Prevention.txt uploaded to chatbot-pro/raw-data/Diabetes_Prevention.txt\n",
      "File ./txt_docs/Diabetes_Mody.txt uploaded to chatbot-pro/raw-data/Diabetes_Mody.txt\n",
      "File ./txt_docs/Updated-NDM-TakeDiabetesToHeart-508-NHLBI.txt uploaded to chatbot-pro/raw-data/Updated-NDM-TakeDiabetesToHeart-508-NHLBI.txt\n",
      "File ./txt_docs/DIABETES DATA.txt uploaded to chatbot-pro/raw-data/DIABETES DATA.txt\n"
     ]
    }
   ],
   "source": [
    "def set_aws_credentials_from_file():\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    # Read the AWS credentials and config files\n",
    "    config.read(os.path.expanduser('~/.aws/credentials'))\n",
    "    aws_access_key_id = config.get('default', 'aws_access_key_id')\n",
    "    aws_secret_access_key = config.get('default', 'aws_secret_access_key')\n",
    "\n",
    "    config.read(os.path.expanduser('~/.aws/config'))\n",
    "    region_name = config.get('default', 'region')\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key_id\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_access_key\n",
    "    os.environ['AWS_REGION'] = region_name\n",
    "\n",
    "def main():\n",
    "    set_aws_credentials_from_file()\n",
    "\n",
    "    folder_path = \"./txt_docs/\"  # Local folder path\n",
    "    bucket_name = \"chatbot-pro\"\n",
    "    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    region_name = os.getenv('AWS_REGION')\n",
    "    s3_folder = \"raw-data/\"  # S3 folder path\n",
    "\n",
    "    # Check if AWS credentials are available\n",
    "    if not aws_access_key_id or not aws_secret_access_key or not region_name:\n",
    "        print(\"AWS credentials are not set in the environment variables.\")\n",
    "        return\n",
    "\n",
    "    # Upload all .txt files to S3\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            full_file_path = os.path.join(folder_path, file_name)\n",
    "            object_name = os.path.join(s3_folder, file_name)  # S3 object name with folder path\n",
    "            upload_to_s3(\n",
    "                file_name=full_file_path,\n",
    "                bucket=bucket_name,\n",
    "                object_name=object_name,\n",
    "                aws_access_key_id=aws_access_key_id,\n",
    "                aws_secret_access_key=aws_secret_access_key,\n",
    "                region_name=region_name\n",
    "            )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7cfab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
